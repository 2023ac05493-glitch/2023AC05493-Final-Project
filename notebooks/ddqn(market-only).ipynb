{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m8wcrIwDkl4",
        "outputId": "1ecbdc1b-2fb6-42a5-a956-de8da1a4c8de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching Nifty-50 & VIX...\n",
            "Training DDQN (Market-only 3D)...\n",
            "Episode 0, Avg Reward: -651648.6319, ε: 0.010\n",
            "Episode 100, Avg Reward: -256211259252.1356, ε: 0.010\n",
            "Episode 200, Avg Reward: -258222036681.0588, ε: 0.010\n",
            "Episode 300, Avg Reward: -263933164710.7338, ε: 0.010\n",
            "Episode 400, Avg Reward: -266046926391.7320, ε: 0.010\n",
            "Episode 500, Avg Reward: -266046932808.3934, ε: 0.010\n",
            "Episode 600, Avg Reward: -270248823744.6336, ε: 0.010\n",
            "Episode 700, Avg Reward: -270248549014.8958, ε: 0.010\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 166\u001b[39m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m agent, test_features, test_vars, dates[split:]\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Run & Evaluate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m agent, test_features, test_vars, test_dates = \u001b[43mtrain_ddqn_market_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== TEST RESULTS (95\u001b[39m\u001b[33m%\u001b[39m\u001b[33m VaR) ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m mape = \u001b[32m0\u001b[39m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mtrain_ddqn_market_only\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    154\u001b[39m     agent.remember(state, action, reward, next_state, done)\n\u001b[32m    155\u001b[39m     episode_reward += reward\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m episode % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m    159\u001b[39m     agent.update_target()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mDDQNVarAgent.replay\u001b[39m\u001b[34m(self, batch_size)\u001b[39m\n\u001b[32m     77\u001b[39m loss.backward()\n\u001b[32m     78\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.q_net.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epsilon > \u001b[38;5;28mself\u001b[39m.epsilon_min:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m.epsilon *= \u001b[38;5;28mself\u001b[39m.epsilon_decay\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2023AC05493-Project\\Lib\\site-packages\\torch\\optim\\optimizer.py:526\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    521\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    522\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    523\u001b[39m             )\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m# pyrefly: ignore [invalid-param-spec]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2023AC05493-Project\\Lib\\site-packages\\torch\\optim\\optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2023AC05493-Project\\Lib\\site-packages\\torch\\optim\\adam.py:248\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    236\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    238\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    239\u001b[39m         group,\n\u001b[32m    240\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m         state_steps,\n\u001b[32m    246\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2023AC05493-Project\\Lib\\site-packages\\torch\\optim\\optimizer.py:151\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2023AC05493-Project\\Lib\\site-packages\\torch\\optim\\adam.py:970\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    968\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2023AC05493-Project\\Lib\\site-packages\\torch\\optim\\adam.py:476\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    472\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(\n\u001b[32m    473\u001b[39m             grad, grad, value=cast(\u001b[38;5;28mfloat\u001b[39m, \u001b[32m1\u001b[39m - beta2)\n\u001b[32m    474\u001b[39m         )\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m.addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    479\u001b[39m     step = step_t\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import random\n",
        "import yfinance as yf\n",
        "\n",
        "class DuelingDDQN(nn.Module):\n",
        "    \"\"\"Dueling DDQN for VaR prediction - 3D market state only\"\"\"\n",
        "    def __init__(self, state_dim=3, action_dim=9):  # 9 quantiles: 1-99%\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Dueling streams\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(64, 32), nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            nn.Linear(64, 32), nn.ReLU(),\n",
        "            nn.Linear(32, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.embedding(x)\n",
        "        value = self.value_stream(feat)\n",
        "        advantage = self.advantage_stream(feat)\n",
        "        return value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "\n",
        "class DDQNVarAgent:\n",
        "    def __init__(self, state_dim=3, action_dim=9):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.q_net = DuelingDDQN(state_dim, action_dim)\n",
        "        self.target_net = DuelingDDQN(state_dim, action_dim)\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=0.001)\n",
        "        self.memory = deque(maxlen=5000)\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.quantile_levels = np.array([0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.random() <= self.epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.q_net(state)\n",
        "        return q_values.argmax(1).item()\n",
        "\n",
        "    def replay(self, batch_size=32):\n",
        "        if len(self.memory) < batch_size: return\n",
        "\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        states = torch.FloatTensor(np.array([e[0] for e in minibatch]))\n",
        "        actions = torch.LongTensor([e[1] for e in minibatch])\n",
        "        rewards = torch.FloatTensor([e[2] for e in minibatch])\n",
        "        next_states = torch.FloatTensor(np.array([e[3] for e in minibatch]))\n",
        "        dones = torch.BoolTensor([e[4] for e in minibatch])\n",
        "\n",
        "        current_q = self.q_net(states).gather(1, actions.unsqueeze(1))\n",
        "        next_q = self.target_net(next_states).max(1)[0].detach()\n",
        "        target_q = rewards + (0.99 * next_q * ~dones)\n",
        "\n",
        "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "    def predict_var(self, state, confidence=0.95):\n",
        "        self.q_net.eval()\n",
        "        with torch.no_grad():\n",
        "            state_t = torch.FloatTensor(state).unsqueeze(0)\n",
        "            qvals = self.q_net(state_t)\n",
        "            idx = np.argmin(np.abs(self.quantile_levels - confidence))\n",
        "            var_pred = abs(qvals[0, idx].item())\n",
        "        self.q_net.train()\n",
        "        return var_pred\n",
        "\n",
        "# 3D Feature Engineering (No News)\n",
        "def create_market_features():\n",
        "    print(\"Fetching Nifty-50 & VIX...\")\n",
        "    nifty = yf.download('^NSEI', start='2020-01-01', end='2026-01-27', progress=False)\n",
        "    vix = yf.download('^INDIAVIX', start='2020-01-01', end='2026-01-27', progress=False)\n",
        "\n",
        "    df = pd.concat([nifty['Close'], vix['Close']], axis=1, keys=['nifty_close', 'vix'])\n",
        "    df = df.bfill()\n",
        "\n",
        "    # 3D features exactly as Nifty50 report\n",
        "    df['returns'] = df['nifty_close'].pct_change()\n",
        "    df['log_price_norm'] = np.log(df['nifty_close']) - np.log(df['nifty_close']).median()\n",
        "    df['vix_norm'] = (df['vix'] - df['vix'].mean()) / df['vix'].std()\n",
        "\n",
        "    # Stack into 3D state vectors\n",
        "    features = np.column_stack([\n",
        "        df['returns'].values[1:],      # lag returns\n",
        "        df['vix_norm'].values[1:],     # VIX z-score\n",
        "        df['log_price_norm'].values[1:] # normalized price\n",
        "    ])\n",
        "\n",
        "    # True VaR labels (rolling 252-day 5th percentile)\n",
        "    true_var = []\n",
        "    for i in range(252, len(df)):\n",
        "        window = df['returns'].iloc[i-252:i].dropna()\n",
        "        true_var.append(abs(np.percentile(window, 5)))\n",
        "\n",
        "    return features[251:-1], np.array(true_var), df.index[252:]\n",
        "\n",
        "# Training (10000 episodes as per report)\n",
        "def train_ddqn_market_only():\n",
        "    features, true_vars, dates = create_market_features()\n",
        "\n",
        "    split = int(0.8 * len(features))\n",
        "    train_features, test_features = features[:split], features[split:]\n",
        "    train_vars, test_vars = true_vars[:split], true_vars[split:]\n",
        "\n",
        "    agent = DDQNVarAgent(state_dim=3, action_dim=9)\n",
        "\n",
        "    print(\"Training DDQN (Market-only 3D)...\")\n",
        "    for episode in range(10000):\n",
        "        episode_reward = 0\n",
        "\n",
        "        # Single episode = full training sequence\n",
        "        for i in range(len(train_features)):\n",
        "            state = train_features[i]\n",
        "            true_var = train_vars[i]\n",
        "\n",
        "            action = agent.act(state)\n",
        "            qvals = agent.q_net(torch.FloatTensor(state).unsqueeze(0))\n",
        "            pred_var = abs(qvals[0, action].item())\n",
        "\n",
        "            # Reward: negative MAPE\n",
        "            reward = -abs((true_var - pred_var) / true_var)\n",
        "            next_state = train_features[min(i+1, len(train_features)-1)]\n",
        "            done = (i == len(train_features) - 1)\n",
        "\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            episode_reward += reward\n",
        "            agent.replay()\n",
        "\n",
        "        if episode % 1000 == 0:\n",
        "            agent.update_target()\n",
        "            avg_reward = episode_reward / len(train_features)\n",
        "            print(f\"Episode {episode}, Avg Reward: {avg_reward:.4f}, ε: {agent.epsilon:.3f}\")\n",
        "\n",
        "    return agent, test_features, test_vars, dates[split:]\n",
        "\n",
        "# Run & Evaluate\n",
        "agent, test_features, test_vars, test_dates = train_ddqn_market_only()\n",
        "\n",
        "print(\"\\n=== TEST RESULTS (95% VaR) ===\")\n",
        "mape = 0\n",
        "for i, (state, true_var) in enumerate(zip(test_features[:20], test_vars[:20])):\n",
        "    pred_var = agent.predict_var(state, confidence=0.95)\n",
        "    error_pct = abs((true_var - pred_var) / true_var) * 100\n",
        "    mape += error_pct\n",
        "    print(f\"{test_dates[i].date()}: True={true_var:.4f}, Pred={pred_var:.4f}, Error={error_pct:.1f}%\")\n",
        "\n",
        "print(f\"\\nDDQN(3D Market) MAPE: {mape/len(test_vars):.1f}%\")\n",
        "print(\"Expected: ~17.8% (report benchmark with news)\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
